{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working with text!</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Simple sentiment analysis</h3>\n",
    "Compute the proportion of positive and negative words in a text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Corpus of positive and negative words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's see how the lists look\n",
    "import requests\n",
    "words = requests.get(p_url).content.decode('latin-1')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(url):\n",
    "    import requests\n",
    "    words = requests.get(url).content.decode('latin-1')\n",
    "\n",
    "    word_list = words.split('\\n')\n",
    "    index = 0\n",
    "    # Loop through the words/lines and remove what's not a word\n",
    "    while index < len(word_list):\n",
    "        word = word_list[index]\n",
    "        if ';' in word or not word:\n",
    "            word_list.pop(index)\n",
    "        else:\n",
    "            index+=1\n",
    "    return word_list\n",
    "\n",
    "#Get lists of positive and negative words\n",
    "positive_words = get_words(p_url)\n",
    "negative_words = get_words(n_url)\n",
    "print(positive_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interesting analysis of presidential candidates debates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    if word and ((word[-1] >= 'a' and word[-1]<='z') or (word[-1] >= 'A' and word[-1]<='Z')):\n",
    "        return word\n",
    "    elif word:\n",
    "        return word[:-1]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = \"word.\"\n",
    "print(word[-1] >= 'a')\n",
    "#print(ord(word[-1]), ord('a'), ord('b'))\n",
    "#(word[-1] >= 'a' and word[-1]<='z') or (word[-1] >= 'A' and word[-1]<='Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get debate texts\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "for text in debate_texts:\n",
    "    with open('Class 9 - data/'+text,'r') as f:\n",
    "        data = f.read()\n",
    "    words = data.split()\n",
    "    cpos=0\n",
    "    cneg=0\n",
    "    for word in words:\n",
    "        word = remove_punctuation(word)\n",
    "        if word in positive_words:\n",
    "            cpos += 1\n",
    "        if word in negative_words:\n",
    "            cneg += 1\n",
    "    total_words = len(words)\n",
    "    ppct = cpos/total_words * 100\n",
    "    npct = cneg/total_words * 100\n",
    "    pos_neg_ratio = cpos/cneg\n",
    "    print(\"%-10s\\t%7d\\t%2.2f\\t%2.2f\\t%2.2f\"%(text,len(words),ppct,npct,pos_neg_ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's see if the debate gets more positive or negative as it proceeds</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's read all the texts\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "all_texts = dict()\n",
    "for text in debate_texts:\n",
    "    with open('Class 9 - data/'+text,'r') as f:\n",
    "        data = f.read()\n",
    "        all_texts[text] = data\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>nltk: Python's natural language toolkit</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install nltk --upgrade\n",
    "#import nltk #The nltk library\n",
    "#nltk.download() or nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>token:</b> A sequence (or group) of characters of interest. For e.g., in the below analysis, a token = a word\n",
    "<li>Generally: A token is the base unit of analysis</li>\n",
    "<li>So, the first step is to convert text into tokens and nltk text object</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>sent_tokenize: constructs sentences</i><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "text = all_texts[debate_texts[4]]\n",
    "sents = sent_tokenize(text)\n",
    "pos_neg_ratio_list = list()\n",
    "cumpos=0\n",
    "cumneg=0\n",
    "\n",
    "for sent in sents:\n",
    "    for word in sent.split():\n",
    "        if word in positive_words:\n",
    "            cumpos+=1\n",
    "        if word in negative_words:\n",
    "            cumneg+=1\n",
    "    try:\n",
    "        pos_neg_ratio_list.append(cumpos/cumneg)\n",
    "    except:\n",
    "        pos_neg_ratio_list.append(0)\n",
    "plt.plot(pos_neg_ratio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's functionalize this\n",
    "def get_pos_neg_ratios(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    pos_neg_ratio_list = list()\n",
    "    cumpos=0\n",
    "    cumneg=0\n",
    "    for sent in sents:\n",
    "        for word in sent.split():\n",
    "            if word in positive_words:\n",
    "                cumpos+=1\n",
    "            if word in negative_words:\n",
    "                cumneg+=1\n",
    "        try:\n",
    "            pos_neg_ratio_list.append(cumpos/cumneg)\n",
    "        except:\n",
    "            pos_neg_ratio_list.append(0)\n",
    "    return pos_neg_ratio_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now we can do a side by side comparison for all three debates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's read all the texts\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "all_texts = dict()\n",
    "for text in debate_texts:\n",
    "    with open('Class 9 - data/'+text,'r') as f:\n",
    "        data = f.read()\n",
    "        all_texts[text] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set up side by side plots\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 3\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))    \n",
    "for i in range(len(debate_texts)):\n",
    "    text = all_texts[debate_texts[i]]\n",
    "    disp_list = get_pos_neg_ratios(text)\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.set_title(debate_texts[i])\n",
    "    ax.plot(disp_list)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating a wordcloud.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's collect all the text into strings\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "all_texts = dict()\n",
    "for text in debate_texts:\n",
    "    with open('Class 9 - data/'+text,'r') as f:\n",
    "        data = f.read()\n",
    "        all_texts[text] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>STOPWORDS: a list of common words that word cloud ignores\n",
    "<li>If you need to ignore other words, you must explicitly delete them from your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000).generate(all_texts['clinton.1'])\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Let's remove the speaker markers from the texts\n",
    "<li>Then we'll render our wordclouds in a 3x2 grid so that we can compare debate to debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Remove unwanted words\n",
    "DELETE_WORDS = ['CLINTON','TRUMP','HOLT','WARREN','COOPER','RADDATZ','said']\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "#Set up side by side clouds\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 3\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "for i in range(0,len(debate_texts)):\n",
    "    text_string = remove_words(all_texts[debate_texts[i]])\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.set_title(debate_texts[i])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Let's junk all small words\n",
    "<li>We need to do some pre-processing of the text to see what words are junkable. Add those to DELETE_WORDS\n",
    "<li>We'll also control the number of words that we're going to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Remove unwanted words\n",
    "DELETE_WORDS = ['CLINTON','TRUMP','HOLT','WARREN','COOPER','RADDATZ']\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "#Remove short words\n",
    "def remove_short_words(text_string,min_length = 5):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n",
    "\n",
    "\n",
    "#Set up side by side clouds\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 3\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "for i in range(0,len(debate_texts)):\n",
    "    text_string = remove_words(all_texts[debate_texts[i]])\n",
    "    text_string = remove_short_words(text_string)\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.set_title(debate_texts[i])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000,max_words=20).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "#so that graphs render inside the browser\n",
    "%matplotlib inline \n",
    "import numpy as np #because numpy underlies everything and we never know when we'll need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_text = \"\"\"\n",
    "polonius to Laertes: Neither a borrower nor a lender be, \n",
    "For loan oft loses both itself and friend, \n",
    "And borrowing dulls the edge of husbandry.\n",
    "This above all: to thine own self be true,\n",
    "And it must follow, as the night the day,\n",
    "Thou canst not then be false to any man.\n",
    "Farewell. My blessing season this in thee.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<i>word_tokenize: constructs word tokens</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "sentences = nltk.Text(sent_tokenize(random_text))\n",
    "print(len(sentences))\n",
    "words = nltk.Text(word_tokenize(random_text))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>nltk contains a large number of pre-tokenized data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text4[0:100]\n",
    "len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We can import our own texts into (our!) nltk corpus using the PlainTextCorpusReader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root='Class 9 - data'\n",
    "clinton_filename='clinton.*'\n",
    "trump_filename='trump.*'\n",
    "clinton_data = PlaintextCorpusReader(corpus_root,clinton_filename,encoding='latin-1')\n",
    "trump_data = PlaintextCorpusReader(corpus_root,trump_filename,encoding='latin-1')\n",
    "clinton_content = nltk.Text(clinton_data.words())\n",
    "trump_content = nltk.Text(trump_data.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Basic analysis</h3>\n",
    "\n",
    "<b>Complexity</b>. Judging a text by how hard it is to read!\n",
    "We can look at the average word length, the average sentence length, and the number of different\n",
    "words used (vocabulary) as a percentage of the length of the text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Complexity factors:</h4>\n",
    "<li>average word length: longer words adds to complexity\n",
    "<li>average sentence length: longer sentences are more complex (unless the text is rambling!)\n",
    "<li>vocabulary: the ratio of unique words used to the total number of words (more variety, more complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_version=' '.join(text4)\n",
    "num_chars=len(string_version)\n",
    "num_words=len(word_tokenize(string_version))\n",
    "num_sentences=len(sent_tokenize(string_version))\n",
    "vocab = {x.lower() for x in word_tokenize(string_version)}\n",
    "print(num_chars,int(num_chars/num_words),int(num_words/num_sentences),len(vocab)/num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's functionalize this for reuse\n",
    "def get_complexity(text):\n",
    "    num_chars=len(text)\n",
    "    num_words=len(word_tokenize(text))\n",
    "    num_sentences=len(sent_tokenize(text))\n",
    "    vocab = {x.lower() for x in word_tokenize(text)}\n",
    "    return len(vocab),int(num_chars/num_words),int(num_words/num_sentences),len(vocab)/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#And test it\n",
    "print(get_complexity(string_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Wonder how that compares with Polonious' speech above?\n",
    "print(get_complexity(random_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hmm. Polonius speech uses more vocabulary than do our Presidents \n",
    "# (but uses shorter words and shorter sentences)!\n",
    "# What about Jane Austen and monty python?\n",
    "string_version_austen=' '.join(text2)\n",
    "print(get_complexity(string_version_austen))\n",
    "string_version_monty=' '.join(text6)\n",
    "print(get_complexity(string_version_monty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's compare austen, montypython, past presidents, and our two current candidates for president</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = (('Presidents',text4),('Austen',text2),('MontyPython',text6),('Clinton',clinton_content),('Trump',trump_content)) \n",
    "print('%14s\\t%14s\\t%14s\\t%14s\\t%14s'%('Text','Vocabulary','Word Length','Sentence Length','Word Variety'))\n",
    "for text in texts:\n",
    "    vocab,word_len,sen_len,variety = get_complexity(' '.join(text[1]))\n",
    "    print('%14s\\t%14d\\t%14d\\t%14d\\t%7.2f'%(text[0],vocab,word_len,sen_len,variety))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Wonder if the complexity of Presidential inaugural addresses have changed over time\n",
    "\n",
    "early_text = string_version[:392533] #approximately the first half\n",
    "later_text = string_version[392533:] #approximately the second half\n",
    "print(get_complexity(early_text))\n",
    "print(get_complexity(later_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sentences are getting shorter but word size and vocab ratio seem unchanged</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can drill down and look at sentence length for each individual inaugural address in temporal order to see how sentence length has changed from Washington to Obama (2009)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import individual files from the corpus\n",
    "from nltk.corpus import inaugural\n",
    "sentence_lengths = list()\n",
    "print(inaugural.fileids())\n",
    "for fileid in inaugural.fileids():\n",
    "    sentence_lengths.append(get_complexity(' '.join(inaugural.words(fileid)))[2])\n",
    "plt.plot(sentence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see if 'debate practice' helped our candidates improve the complexity of their sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fileid in clinton_data.fileids():\n",
    "    print(fileid,get_complexity(' '.join(clinton_data.words(fileid))))\n",
    "    \n",
    "for fileid in trump_data.fileids():\n",
    "    print(fileid,get_complexity(' '.join(trump_data.words(fileid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice problem 1 ...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice problem 2...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part of Speech tagging</h2>\n",
    "<h4>POS tagging is useful because we get some sense of how a word is used</h4>\n",
    "<h4>For example: We might want to see if the candidates differ on how they describe stuff. More positive or more negative</h4>\n",
    "<h4>And that may be different from the  positive/negative ratios of the overall text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag('I had a great spring break. I travelled with my family to Costa Rica and avoided the snow storm in New York'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get a list of all tags for reference\n",
    "import nltk\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = nltk.pos_tag(text4)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see how they differ on the use of adjectives (JJ, JJR, JJS) </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "for text in debate_texts:\n",
    "    with open('Class 9 - data/'+text,'r') as f:\n",
    "        data = f.read()\n",
    "    words = nltk.pos_tag(data.split())\n",
    "    cpos=0\n",
    "    cneg=0\n",
    "    for word in words:\n",
    "        if not word[1] in ['JJ','JJR','JJS']:\n",
    "            continue\n",
    "        if word[0] in positive_words:\n",
    "            cpos += 1\n",
    "        if word[0] in negative_words:\n",
    "            cneg += 1\n",
    "    total_words = len(words)\n",
    "    ppct = cpos/total_words * 100\n",
    "    npct = cneg/total_words * 100\n",
    "    pos_neg_ratio = cpos/cneg\n",
    "    print(\"%10s\\t%7d\\t%2.2f\\t%2.2f\\t%2.2f\"%(text,len(words),ppct,npct,pos_neg_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For comparison\n",
    "debate_texts = ['clinton.1','trump.1','clinton.2','trump.2','clinton.3','trump.3']\n",
    "for text in debate_texts:\n",
    "    with open('Class 9 - data/'+text,'r') as f:\n",
    "        data = f.read()\n",
    "    words = data.split()\n",
    "    cpos=0\n",
    "    cneg=0\n",
    "    for word in words:\n",
    "        word = remove_punctuation(word)\n",
    "        if word in positive_words:\n",
    "            cpos += 1\n",
    "        if word in negative_words:\n",
    "            cneg += 1\n",
    "    total_words = len(words)\n",
    "    ppct = cpos/total_words *100\n",
    "    npct = cneg/total_words * 100\n",
    "    pos_neg_ratio = cpos/cneg\n",
    "    print(\"%10s\\t%7d\\t%2.2f\\t%2.2f\\t%2.2f\"%(text,len(words),ppct,npct,pos_neg_ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding entity names in a text</h3>\n",
    "<li>ne_chunk_sents takes a list of POS tagged sentences and returns a named entity tree with groups of NNP's and sentence fragments as the leaves\n",
    "<li>NNP groups are extracted by seeing if they occur in the same sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entity_names(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    print(sentences)\n",
    "    #tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    #print(tokenized_sentences)\n",
    "    #tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    #print(tagged_sentences)\n",
    "    #chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    #for c in chunked_sentences:\n",
    "    #    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clinton in debate 1\n",
    "get_entity_names(all_texts['clinton.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entity_names(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    \n",
    "    def extract_entity_names(t):\n",
    "        entity_names = []\n",
    "\n",
    "        if hasattr(t, 'label') and t.label:\n",
    "            if t.label() == 'NE':\n",
    "                entity_names.append(' '.join([child[0] for child in t]))\n",
    "            else:\n",
    "                for child in t:\n",
    "                    entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "        return entity_names\n",
    "\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "    return set(entity_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clinton in debate 1\n",
    "get_entity_names(all_texts['clinton.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see what the differences between the two candidates is on named entities</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_texts = all_texts['clinton.1'] + all_texts['clinton.2'] + all_texts['clinton.3']\n",
    "clinton_ne = get_entity_names(clinton_texts)\n",
    "trump_texts = all_texts['trump.1'] + all_texts['trump.2'] + all_texts['trump.3']\n",
    "trump_ne = get_entity_names(trump_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_ne - trump_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_ne - clinton_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Frequency Distributions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_clinton = FreqDist(clinton_content)\n",
    "fdist_trump = FreqDist(trump_content)\n",
    "fdist_clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's look at the top 50 words\n",
    "fdist_clinton.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Not very helpful! Let's see what proportion of Clinton words are in the top 50</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(clinton_content))\n",
    "fdist_clinton.plot(50,cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's get rid of common words\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)\n",
    "#fdist_clinton = FreqDist(clinton_content)\n",
    "#for word in stop:\n",
    "#    if fdist_clinton.get(word):\n",
    "#        del fdist_clinton[word]\n",
    "#        \n",
    "#len(fdist_clinton)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's functionalize this\n",
    "def fdist_stop_words(corpus,delete_words):\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in delete_words:\n",
    "        stop.add(word)\n",
    "    fdist = FreqDist(corpus)\n",
    "    for word in stop:\n",
    "        if fdist.get(word):\n",
    "            del fdist[word]\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_stop_words(clinton_content,DELETE_WORDS).plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DELETE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Not helpful again. Let's look only at long words\n",
    "clinton_long = [x.lower() for x in clinton_content if len(x)>8 and x.isalpha()]\n",
    "trump_long = [x.lower() for x in trump_content if len(x)>8 and x.isalpha()]\n",
    "DELETE_WORDS = ['something','everything']\n",
    "    \n",
    "fdist_clinton = fdist_stop_words(clinton_long,DELETE_WORDS)\n",
    "fdist_trump = fdist_stop_words(trump_long,DELETE_WORDS)\n",
    "fdist_trump.most_common(10)\n",
    "#fdist_trump.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_clinton.most_common(10)\n",
    "fdist_clinton.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can use these distributions to look at differences between the candidates</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_words = {w for w in fdist_clinton if fdist_clinton[w]>10}\n",
    "trump_words = {w for w in fdist_trump if fdist_trump[w]>10}\n",
    "print(clinton_words - trump_words)\n",
    "trump_words - clinton_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can mean different things in different contexts so let's try a bit of context analysis<p>\n",
    "<b>Concordance</b>: Occurrences of a word with some context (text around the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_content.concordance('obamacare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_content.concordance('obamacare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can look at who uses a word in a positive context and who in a negative context</h4>\n",
    "But we have to write our own concordance function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concordance(text,word,width):\n",
    "    locs = [loc for loc, x in enumerate(text) if x.lower() == word.lower()]\n",
    "    fragments = list()\n",
    "    for loc in locs:\n",
    "        start = max(0,loc-width)\n",
    "        end = min(len(text),loc+width)\n",
    "        fragments.append(' '.join(text[start:end]))\n",
    "    return fragments\n",
    "concordance(trump_content,'immigrants',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we can write a function to get sentiment for a word</h4>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_concordance_sentiment(text,word,width):\n",
    "    sentences = concordance(text,word,width)\n",
    "    pos=0\n",
    "    neg=0\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word in positive_words:\n",
    "                pos+=1\n",
    "            if word in negative_words:\n",
    "                neg+=1\n",
    "    if not pos and not neg:\n",
    "        return 1\n",
    "    if pos and not neg:\n",
    "        return 10\n",
    "    else:\n",
    "        return pos/neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'immigrants',10))\n",
    "print(get_concordance_sentiment(trump_content,'immigrants',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'obamacare',10))\n",
    "print(get_concordance_sentiment(trump_content,'obamacare',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'people',10))\n",
    "print(get_concordance_sentiment(trump_content,'people',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'obama',10))\n",
    "print(get_concordance_sentiment(trump_content,'obama',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'donald',10))\n",
    "print(get_concordance_sentiment(trump_content,'hillary',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'i',10))\n",
    "print(get_concordance_sentiment(trump_content,'i',10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Similar. What words appear in the same context as the given word\n",
    "<li>When words are used in the same context, they are likely to give us the sense in which the word is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_content.similar('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_content.similar('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.concordance('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_concordance_sentiment(text4,'people',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_content.similar('americans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_content.similar('americans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Earlier we saw that Clinton and Trump stressed different words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_concordance_sentiment(clinton_content,'government',10))  # She said\n",
    "print(get_concordance_sentiment(trump_content,'politicians',10)) # He said"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dispersion plots show how word use changes over the course of the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_content.dispersion_plot(['she','Secretary','Hillary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_content.dispersion_plot(['he', 'Trump','Donald'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can see how the thinking of our presidents has changed</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"Americans\",'independence','God'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question: The main characters in Sense and Sensibility are Elinor, Marianne, Edward, and Willoughby. \n",
    "#Who are more important to the story, the men or the women?\n",
    "text2.dispersion_plot(['Elinor','Marianne','Edward','Willoughby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How about a comparison between early versus late uses of citizen?\n",
    "early_text_tokens.similar('citizen')\n",
    "print('----')\n",
    "later_text_tokens.similar('citizen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Hmm. states seems to be replaced by nation or country \n",
    "<li>Law and government are out. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Common contexts. The set of contexts that are common for a list of words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_text_tokens.common_contexts(['citizen','government'])\n",
    "print('----')\n",
    "later_text_tokens.common_contexts(['citizen','government'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's look at the inaugural speeches again:</h4>\n",
    "<li>Since the inaugural speeches are temporally arranged, we can see the temporal occurrence and distribution of words over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_text_tokens.similar('duties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "later_text_tokens.similar('duties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.concordance('duties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Generally, we're interested in: the frequency of a word and in the most frequent words\n",
    "</li>Let's look at frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text4)\n",
    "fdist1.most_common(10) #Try 50, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Not very helpful. Let's see what proportion of the words in the top 50 make up the text\n",
    "fdist1.plot(50,cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text4) #Approximately half the words in the text are in the top 50. Most are useless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bi-grams and n-grams</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Individual words don't tell us a whole lot. So we work with pairs, triads, etc. of words that go together.\n",
    "<li>bigram: a pair of words that go togther. 'American people', 'God bless','Soviet Union'\n",
    "<li>Collocations: Frequent bigrams in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_content.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_content.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Collocations are computed by a submodule of nltk. \n",
    "<li>All pairs of words are examined and a strength (using the relative frequency of co-occurence) is assigned to each word.\n",
    "<li>PMI: Pointwise Mutual Information (uses conditional probabilities based on the relative frequency to compute a measure of association)\n",
    "<li>We can use this module to identify all bigrams (and trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = nltk.BigramCollocationFinder.from_words(clinton_content)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "ignored_words = nltk.corpus.stopwords.words('english') + ['CLINTON','...'] \n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.apply_freq_filter(5)\n",
    "print(finder.nbest(bigram_measures.pmi,50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = nltk.BigramCollocationFinder.from_words(trump_content)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "ignored_words = nltk.corpus.stopwords.words('english') + ['LAUGHTER',')',',','CROSSTALK'] #Set up a list of words to ignore\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.apply_freq_filter(5)\n",
    "print(finder.nbest(bigram_measures.pmi,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>By adjusting the window size, nltk can find associations even if there are intervening words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = nltk.BigramCollocationFinder.from_words(trump_content,window_size=10)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "ignored_words = nltk.corpus.stopwords.words('english') + ['LAUGHTER',')',',','CROSSTALK'] #Set up a list of words to ignore\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.apply_freq_filter(5)\n",
    "print(finder.nbest(bigram_measures.pmi,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = nltk.BigramCollocationFinder.from_words(clinton_content,window_size=10)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "ignored_words = nltk.corpus.stopwords.words('english') + ['LAUGHTER',')',',','CROSSTALK'] #Set up a list of words to ignore\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.apply_freq_filter(5)\n",
    "print(finder.nbest(bigram_measures.pmi,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Trigrams: Groups of 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = nltk.TrigramCollocationFinder.from_words(clinton_content)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "sorted(finder.nbest(trigram_measures.pmi,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finder = nltk.TrigramCollocationFinder.from_words(trump_content)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "sorted(finder.nbest(trigram_measures.pmi,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using bigrams and trigrams</h3>\n",
    "<ol>\n",
    "<li>Sentiment analysis on concepts\n",
    "<li>Construct networks of words to identify broader concepts (tune in next week ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conditional Frequency Distributions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>A conditional frequency distribution allows us to compare texts in different categories by counting the occurrence of words in each category (or text)\n",
    "<li>For example, the inaugural addresses are divided into multiple files for each address arranged temporally\n",
    "<li>We can use this time element to count words and see how usage changes across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinton_data.fileids() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd_clinton=nltk.ConditionalFreqDist((target,fileid) \n",
    "                             for fileid in clinton_data.fileids() \n",
    "                             for w in clinton_data.words(fileid)\n",
    "                             for target in ['country','border']\n",
    "                             if w.lower().startswith(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_clinton.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_trump=nltk.ConditionalFreqDist((target,fileid) \n",
    "                             for fileid in trump_data.fileids() \n",
    "                             for w in trump_data.words(fileid)\n",
    "                             for target in ['country','border']\n",
    "                             if w.lower().startswith(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_trump.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_trump.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_clinton.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see how inward/outward looking the nation has gotten since Washington's inauguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inaugural.fileids() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_inaugural=nltk.ConditionalFreqDist((target,fileid) \n",
    "                             for fileid in inaugural.fileids() \n",
    "                             for w in inaugural.words(fileid)\n",
    "                             for target in ['world','nation']\n",
    "                             if w.lower().startswith(target))\n",
    "cfd_inaugural.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>NLTK resources:</h2>\n",
    "<h3>ntlk documentation:</h3> http://www.nltk.org/api/nltk.html\n",
    "<h3>Commands cheat sheet</h3> https://blogs.princeton.edu/etc/files/2014/03/Text-Analysis-with-NLTK-Cheatsheet.pdf\n",
    "<h3>nltk book</h3>http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
